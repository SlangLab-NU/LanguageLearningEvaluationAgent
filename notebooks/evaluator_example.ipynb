{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from typing import Dict\n",
    "import json\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from evaluator.base_evaluator import RAGEvaluator\n",
    "from utils.llm import OpenAIClientLLM\n",
    "from evaluator.prompt_manager import EvaluationType, EvalPromptManager\n",
    "\n",
    "class LLMRelevanceEvaluator(RAGEvaluator):\n",
    "    def pre_process(\n",
    "        self,\n",
    "        question: str,\n",
    "        context: str,\n",
    "        answer: str\n",
    "    ) -> str:\n",
    "        return self.prompt_manager.build_prompt(\n",
    "            question=question,\n",
    "            context=context,\n",
    "            answer=answer,\n",
    "            eval_type=EvaluationType.RELEVANCE  # or make this configurable\n",
    "        )\n",
    "    def call_llm(self, processed_data: str) -> str:\n",
    "        # Execute LLM call with constructed prompt\n",
    "        return self.llm.generate(processed_data)\n",
    "    \n",
    "    def post_process(self, llm_response: str) -> Dict[str, float]:\n",
    "        \"\"\"Parse JSON response into scores dictionary\"\"\"\n",
    "        try:\n",
    "            # Clean response and parse JSON\n",
    "            response_text = llm_response.strip().replace('```json', '').replace('```', '')\n",
    "            result = json.loads(response_text)\n",
    "            \n",
    "            # Normalize scores and flatten structure\n",
    "            scores = {\n",
    "                'score': result.get('score', \n",
    "                           result.get('relevance_score', \n",
    "                           result.get('coherence_score', \n",
    "                           result.get('accuracy_score', 0.0)))),\n",
    "                'confidence': result.get('confidence', 0.0)\n",
    "            }\n",
    "            \n",
    "            # Add additional metrics\n",
    "            for key in result:\n",
    "                if key.endswith('_score') and key != 'score':\n",
    "                    scores[key] = result[key]\n",
    "            \n",
    "            return scores\n",
    "            \n",
    "        except (json.JSONDecodeError, KeyError) as e:\n",
    "            logger.info(f\"Error parsing LLM response: {e}\")\n",
    "            return {\n",
    "                'score': 0.0,\n",
    "                'confidence': 0.0,\n",
    "                'error': str(e)\n",
    "            }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/th/zffq3_bj3115zg2pzfz4hb0m0000gn/T/ipykernel_29483/3281657495.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  a['flatten_doc'] = a.apply(lambda x: \"\\n\".join([f\"`{label}` {sentence}\" for label, sentence in [inner_list for middle_list in x['documents_sentences'] for inner_list in middle_list]]), axis = 1)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "delucionqa = load_dataset(\"rungalileo/ragbench\", \"delucionqa\")\n",
    "df = delucionqa['train'].to_pandas()\n",
    "a = df.head()\n",
    "a['flatten_doc'] = a.apply(lambda x: \"\\n\".join([f\"`{label}` {sentence}\" for label, sentence in [inner_list for middle_list in x['documents_sentences'] for inner_list in middle_list]]), axis = 1)\n",
    "answer = a.iloc[1]['response']\n",
    "documents = a.iloc[1]['flatten_doc']\n",
    "question = a.iloc[1]['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To customize the Uconnect system based on your own preferences, you can follow these steps:\n",
      "\n",
      "1. Press the apps button on the touchscreen to open the app screen.\n",
      "2. Press and hold the selected app, then drag it to replace an existing shortcut in the main menu bar.\n",
      "3. Note that this feature is only available when the vehicle is in PARK. \n",
      "\n",
      "\n",
      "`0a`  Uconnect 4 with 7-inch display and uconnect 4/4c/4c nav with 8.4-inch display press the apps button, then press the settings button on the touchscreen to display the menu setting screen.\n",
      "`0b` In this mode the Uconnect system allows you to access programmable features.\n",
      "`0c` When making a selection, only press one button at a time to enter the desired menu.\n",
      "`0d` Once in the desired menu, press and release the preferred setting option until a check mark appears next to the setting, showing that setting has been selected.\n",
      "`0e` Once the setting is complete, press the X button on the touchscreen to close out of the settings screen.\n",
      "`0f` Pressing the Up or Down Arrow button on the right side of the screen will allow you to toggle up or down through the available settings.\n",
      "`0g` Note: Depending on the vehicle’s options, feature settings may vary.\n",
      "`0h` All settings should be changed with the ignition in the ON/RUN position.\n",
      "`1a`  Drag & Drop Menu Bar The Uconnect features and services in the main menu bar are easily customized for your preference.\n",
      "`1b` Simply follow these steps: press the apps button to open the app screen.\n",
      "`1c` Press and hold, then drag the selected app to replace an existing shortcut in the main menu bar.\n",
      "`1d` Note: This feature is only available if the vehicle is in PARK.\n",
      "`1e` Uconnect 4 with 7-inch display and uconnect 4/4c/4c nav with 8.4-inch display press the apps button, then press the settings button on the touchscreen to display the menu setting screen.\n",
      "`1f` In this mode the Uconnect system allows you to access programmable features.\n",
      "`1g` When making a selection, only press one button at a time to enter the desired menu.\n",
      "`1h` Once in the desired menu, press and release the preferred setting option until a check mark appears next to the setting, showing that setting has been selected.\n",
      "`1i` Once the setting is complete, press the X button on the touchscreen to close out of the settings screen.\n",
      "`1j` Pressing the Up or Down Arrow button on the right side of the screen will allow you to toggle up or down through the available settings.\n",
      "`1k` Note: Depending on the vehicle’s options, feature settings may vary.\n",
      "`1l` All settings should be changed with the ignition in the ON/RUN position.\n",
      "`1m` UCONNECT SETTINGS The Uconnect system uses a combination of buttons on the touchscreen and buttons on the faceplate located on the center of the instrument panel.\n",
      "`1n` These buttons allow you to access and change the Customer Programmable Features.\n",
      "`1o` Many features can vary by vehicle.\n",
      "`1p` Buttons on the faceplate are located below and/or beside the Uconnect system in the center of the instrument panel.\n",
      "`1q` In addition, there is a SCROLL/ENTER control knob located on the right side.\n",
      "`1r` Turn the control knob to scroll through menus and change settings.\n",
      "`1s` Push the center of the control knob one or more times to select or change a setting.\n",
      "`1t` Your Uconnect system may also have SCREEN OFF and MUTE buttons on the faceplate.\n",
      "`1u` Push the SCREEN OFF button on the faceplate to turn off the Uconnect screen.\n",
      "`1v` Push the button again or tap the screen to turn the screen on.\n",
      "`1w` Press the Back Arrow button to exit out of a Menu or certain option on the Uconnect system.\n",
      "`2a`  UCONNECT SETTINGS The Uconnect system uses a combination of buttons on the touchscreen and buttons on the faceplate located on the center of the instrument panel.\n",
      "`2b` These buttons allow you to access and change the Customer Programmable Features.\n",
      "`2c` Many features can vary by vehicle.\n",
      "`2d` Buttons on the faceplate are located below and/or beside the Uconnect system in the center of the instrument panel.\n",
      "`2e` In addition, there is a SCROLL/ENTER control knob located on the right side.\n",
      "`2f` Turn the control knob to scroll through menus and change settings.\n",
      "`2g` Push the center of the control knob one or more times to select or change a setting.\n",
      "`2h` Your Uconnect system may also have SCREEN OFF and MUTE buttons on the faceplate.\n",
      "`2i` Push the SCREEN OFF button on the faceplate to turn off the Uconnect screen.\n",
      "`2j` Push the button again or tap the screen to turn the screen on.\n",
      "`2k` Press the Back Arrow button to exit out of a Menu or certain option on the Uconnect system. \n",
      "\n",
      "\n",
      "how to customize Uconnect system based on my own preferences? \n"
     ]
    }
   ],
   "source": [
    "logger.info(answer, \"\\n\\n\")\n",
    "logger.info(documents,  \"\\n\\n\")\n",
    "logger.info(question, \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "evaluator = LLMRelevanceEvaluator(\n",
    "    llm=OpenAIClientLLM(),\n",
    "    prompt_manager=EvalPromptManager(default_type=EvaluationType.FACTUAL_ACCURACY)\n",
    ")\n",
    "\n",
    "\n",
    "result = evaluator.evaluate(\n",
    "    question=question,\n",
    "    context=documents,\n",
    "    answer=answer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.9, 'confidence': 0.95, 'relevance_score': 0.9}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "class LLMEquivalenceEvaluator(RAGEvaluator):\n",
    "    def pre_process(\n",
    "        self,\n",
    "        question: str|List[str],\n",
    "        context: str|List[str],\n",
    "        answer: str|List[str]\n",
    "    ) -> str:\n",
    "        assert len(answer) == 2\n",
    "        two_line_answer = f\"    1. {answer[0]}\\n    2. {answer[1]}\"\n",
    "        return self.prompt_manager.build_prompt(\n",
    "            question=question,\n",
    "            context=context,\n",
    "            answer=two_line_answer,\n",
    "            eval_type=EvaluationType.ANSWER_EQUIVALENCE\n",
    "        )\n",
    "        \n",
    "    def call_llm(self, processed_data: str) -> str:\n",
    "        # Execute LLM call with constructed prompt\n",
    "        return self.llm.generate(processed_data)\n",
    "    \n",
    "    def post_process(self, llm_response: str) -> Dict[str, float]:\n",
    "        \"\"\"Parse JSON response into scores dictionary\"\"\"\n",
    "        try:\n",
    "            # Clean response and parse JSON\n",
    "            response_text = llm_response.strip().replace('```json', '').replace('```', '')\n",
    "            result = json.loads(response_text)\n",
    "            \n",
    "            scores = {\n",
    "                \"Q1\": 1 if result['Q1'] == 'yes' else 0,\n",
    "                \"Q2\": 1 if result['Q2'] == 'yes' else 0,\n",
    "                \"Q3\": 1 if result['Q3'] == 'yes' else 0,\n",
    "                \"Q4\": 1 if result['Q4'] == 'yes' else 0,\n",
    "            }\n",
    "            \n",
    "            return scores\n",
    "            \n",
    "        except (json.JSONDecodeError, KeyError) as e:\n",
    "            logger.info(f\"Error parsing LLM response: {response_text}\")\n",
    "            return {\n",
    "                \"Q1\": 0, \"Q2\": 0, \"Q3\": 0, \"Q4\": 0,\n",
    "                'error': str(e)\n",
    "            }\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kfmr310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
